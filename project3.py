# -*- coding: utf-8 -*-
"""Project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BIRkALqv77FqSOSeHpvks4F6T_MXDCkB
"""

!wget https://www.dropbox.com/scl/fi/1wdngi227kabrkedmn0gb/archive.zip?rlkey=yqetdf5rfo3ro0m8y9z083gro&dl=0 >> /dev/null
!unzip archive.zip?rlkey=yqetdf5rfo3ro0m8y9z083gro >> /dev/null

import numpy as np
import pandas as pd
import os
import time
import random
import matplotlib.pyplot as plt
import seaborn as sns
import cv2

from glob import glob
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from skimage.feature import hog
from google.colab.patches import cv2_imshow

import PIL
import cv2
import pickle

# Sets up the Segmentation for a piece of data, and Resizes it
image = cv2.imread('Test/00044.png', cv2.IMREAD_GRAYSCALE)
image2 = image
image = cv2.medianBlur(image,5)
check = 1 # Sentinal Value
while(check):
  if(cv2.mean(image)[0] < 60):
    # Darker images have some problems so the If brightens the image, if its not enough, it does it again
    image = image.astype(np.float32)
    image = np.clip(image * 1.5, 0, 255).astype(np.uint8)
    image = image.astype(np.uint8)
  else:
    # The images will occasionally not have enough contrast, this increases the contrast allowing the bights and darks to be better separated.
    image = cv2.equalizeHist(image)
    check = 0

# Does Otsu's thresholding so that it finds an optimal threashold value, and uses that to separate lights and darks.
_, image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

image = image * image2

# for user to see the current image, will be removed in later code.
cv2_imshow(image2)
cv2_imshow(image)
cv2.waitKey(0)
cv2.destroyAllWindows()

svm_y_train = y_train

# Creating blank dataframe to store model scores

df_scores  = pd.DataFrame(columns = ['Model', 'Score', 'Value'])
df_model = pd.DataFrame(columns = ['Model','Accuracy (%)','Time (mins)'])

# Defining SVM model

svm_clf = svm.NuSVC(nu=0.05,kernel='rbf',gamma=0.00001,random_state=121)

# Fitting SVM

tic = time.perf_counter()
svm_X_train = np.resize(X_train, (27446, 3072))
svm_X_test = np.resize(X_val, (12630, 3072))

#svm_X_train = preprocessing.scale(svm_X_train)
#svm_X_test = preprocessing.scale(svm_X_test)

print(y_train.shape)
print(X_train.shape)
print(X_test.shape)

print(svm_y_train.shape)
svm_clf.fit(svm_X_train, svm_y_train)

toc = time.perf_counter()
m_svm, s_svm = divmod((toc - tic), 60)
time_svm=float(str(str(int(m_svm))+"."+str(int(m_svm))))

# Predicting values for test data

y_pred_svm = svm_clf.predict(svm_X_test)

# Calculating recall, precision, f1 score and accuracy of SVM

recall_svm = metrics.recall_score(y_val, y_pred_svm,average='macro')
df_scores.loc[len(df_scores)] = ["SVM","Recall",recall_svm]

precision_svm = metrics.precision_score(y_val, y_pred_svm,average='macro')
df_scores.loc[len(df_scores)] = ["SVM","Precision",precision_svm]

f1_svm = metrics.f1_score(y_val, y_pred_svm,average='macro')
df_scores.loc[len(df_scores)] = ["SVM","F1",f1_svm]

acc_svm=metrics.accuracy_score(y_val ,y_pred_svm)
df_scores.loc[len(df_scores)] = ["SVM","Accuracy",acc_svm]

df_model.loc[len(df_model)] = ["SVM",acc_svm*100,time_svm]
acc_svm